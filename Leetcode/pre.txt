In traditional Q learning , each state and action has a specific Q value which will cause memory problem.
However, in approximate Q -learning we use features to represent similar situations in learning progress.


In this project, we have selected some significant features to experiment, such as the distance to food, the distance to ghost and the status of pacman. After some iterations of training, these parameters can converge into a certain of value.


In this project, we have selected some significant features to experiment, such as the distance to food, the distance to ghost and the status of pacman. After some iterations of training, these parameters can converge into a certain of value.

However, there still exist some problems. First is dead end scenario which means the agents will get trapped in one situation and repeat the action infinitely. The second is that the pacman has no interest in eating capsules. We think the two problems can be solved if we add some extra features and train the model after more iterations. These problems were solved and will be shown in later video.

The second algorithm is MCTS, it’s a common algorithm...

There are three main problems when we implemented this algorithm, they are…<PPT>. Firstly, if we don't adopt pruning measure or something similar, the computation will cost lots of time on simulation in MCTS. Also, it is of vital importance to set a termination condition in simulation The 3rd problem is how to choose perfect parameters to instruct the pacman.

For the first problem, we can <PPT>. And for the second one, we can <PPT>. As for how to get perfect parameters, we just obtained them from practice. Generally speaking, the weight of 'distance to ghost' should be large if the pacman has eaten a capsule and decrease with the capsule time running out. Apart from this, in practice, when the carrying points of an offensive pacman exceeds a threshold it should be backing home, naturally, the weight of 'backing home' should be high in this situation.